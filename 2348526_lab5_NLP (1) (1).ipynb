{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjy04zeT5bOb",
        "outputId": "cb86ac2b-28d6-47dd-cba6-503da4d4bc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "antonyms:The function antonyms(word) aims to retrieve antonyms for a given word by utilizing the WordNet lexical database. It first initializes an empty list anto to store the antonyms found. Then, it iterates over each synset of the input word in WordNet, exploring different senses or meanings associated with the word"
      ],
      "metadata": {
        "id": "5KS_I8HE9yCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "M9l_X2EK6FtL"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def antonyms(word):\n",
        "  anto = []\n",
        "  for sys in wordnet.synsets(word):\n",
        "    for lem in sys.lemmas():\n",
        "      if lem.antonyms():\n",
        "        anto = lem.antonyms()\n",
        "  return anto\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9goHClMy6ON0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = input(\"Enter the word\")\n",
        "wordsys = antonyms(word)\n",
        "\n",
        "if(antonyms):\n",
        "  print(\"The antonym is \",wordsys)\n",
        "else:\n",
        "  print(\"The antonym for the word doesnot exist\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PhFjyhR8osk",
        "outputId": "d77dba10-28c4-4f78-ff5e-5dbea0ff81e4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the wordbelief\n",
            "The antonym is  [Lemma('unbelief.n.01.unbelief')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a program for stemming non-English words."
      ],
      "metadata": {
        "id": "MSbvi5NNM7Ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Snowball Stemmer is an algorithm used for stemming words, which means reducing them to their root or base form."
      ],
      "metadata": {
        "id": "yyPdaaHMOaSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "xuFxVuPk8uqp"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmer_english(words,language):\n",
        "  stemmer = SnowballStemmer(language)\n",
        "  stemm = stemmer.stem(words)\n",
        "  return stemm"
      ],
      "metadata": {
        "id": "jJBZE6358s2w"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang = input(\"Enter the language\")\n",
        "words = input(\"Enter the word\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruHt6d5PDyvL",
        "outputId": "03f4a82d-413d-4f54-ce86-07d23448a5ce"
      },
      "execution_count": 75,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the languagespanish\n",
            "Enter the wordcorriendo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The stememed word is :\", stemmer_english(words,lang))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4e0_2GiE1eb",
        "outputId": "62aa4558-621e-43ea-935d-1ed4d3554548"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The stememed word is : corr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a program for lemmatizing words Using WordNet (Use all type of stemmers for the\n",
        "comparison)."
      ],
      "metadata": {
        "id": "kdLqws6UM4WS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WordNetLemmatizer determines the lemma, or base form, of a word based on its part of speech. The PorterStemmer and LancasterStemmer algorithms perform stemming by stripping affixes to obtain the root form, though LancasterStemmer tends to be more aggressive. The SnowballStemmer, an enhancement over Porter algorithm, offers improved accuracy and multilingual support, making it versatile for stemming tasks in NLP."
      ],
      "metadata": {
        "id": "JfiA2Ay5Oxn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer\n"
      ],
      "metadata": {
        "id": "BT8nI4RFFHQT"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "kFOiZBCRGAKV"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"played\", \"plays\", \"running\", \"ran\", \"playing\"]\n",
        "for word in words:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)\n",
        "    porter_stem = porter_stemmer.stem(word)\n",
        "    lancaster_stem = lancaster_stemmer.stem(word)\n",
        "    snowball_stem = snowball_stemmer.stem(word)\n",
        "    print(f\"{word}\\t\\tlemmatised word:{lemmatized_word}\\t\\tporter stemming:{porter_stem}\\t\\tlancaster stemming:{lancaster_stem}\\t\\tSnowball stemming:{snowball_stem}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W73OqgPcGCws",
        "outputId": "93dfe404-358f-4621-a170-8eab09b0eb22"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\t\tlemmatised word:running\t\tporter stemming:run\t\tlancaster stemming:run\t\tSnowball stemming:run\n",
            "played\t\tlemmatised word:played\t\tporter stemming:play\t\tlancaster stemming:play\t\tSnowball stemming:play\n",
            "plays\t\tlemmatised word:play\t\tporter stemming:play\t\tlancaster stemming:play\t\tSnowball stemming:play\n",
            "running\t\tlemmatised word:running\t\tporter stemming:run\t\tlancaster stemming:run\t\tSnowball stemming:run\n",
            "ran\t\tlemmatised word:ran\t\tporter stemming:ran\t\tlancaster stemming:ran\t\tSnowball stemming:ran\n",
            "playing\t\tlemmatised word:playing\t\tporter stemming:play\t\tlancaster stemming:play\t\tSnowball stemming:play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a program to differentiate stemming and lemmatizing words."
      ],
      "metadata": {
        "id": "aIC9N48IMpF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Initialize WordNet lemmatizer and Porter stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"The quick brown foxes are running in the fields.\",\n",
        "    \"He played football yesterday and he is playing again today.\"\n",
        "]\n",
        "\n",
        "print(\"Sentence\\t\\tLemmatized\\t\\tStemmed\")\n",
        "\n",
        "\n",
        "\n",
        "for sentence in sentences:\n",
        "\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "\n",
        "    print(f\"{sentence}\\n\")\n",
        "    for i in range(len(words)):\n",
        "        print(f\"{words[i]:15} Lemmatised word:{lemmatized_words[i]:15} Stemmed word:{stemmed_words[i]:15}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3d-RLJsGU5Y",
        "outputId": "f2199254-c06d-4c97-93de-1c3a77a8739c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence\t\tLemmatized\t\tStemmed\n",
            "The quick brown foxes are running in the fields.\n",
            "\n",
            "The             Lemmatised word:The             Stemmed word:the            \n",
            "quick           Lemmatised word:quick           Stemmed word:quick          \n",
            "brown           Lemmatised word:brown           Stemmed word:brown          \n",
            "foxes           Lemmatised word:fox             Stemmed word:fox            \n",
            "are             Lemmatised word:are             Stemmed word:are            \n",
            "running         Lemmatised word:running         Stemmed word:run            \n",
            "in              Lemmatised word:in              Stemmed word:in             \n",
            "the             Lemmatised word:the             Stemmed word:the            \n",
            "fields          Lemmatised word:field           Stemmed word:field          \n",
            ".               Lemmatised word:.               Stemmed word:.              \n",
            "He played football yesterday and he is playing again today.\n",
            "\n",
            "He              Lemmatised word:He              Stemmed word:he             \n",
            "played          Lemmatised word:played          Stemmed word:play           \n",
            "football        Lemmatised word:football        Stemmed word:footbal        \n",
            "yesterday       Lemmatised word:yesterday       Stemmed word:yesterday      \n",
            "and             Lemmatised word:and             Stemmed word:and            \n",
            "he              Lemmatised word:he              Stemmed word:he             \n",
            "is              Lemmatised word:is              Stemmed word:is             \n",
            "playing         Lemmatised word:playing         Stemmed word:play           \n",
            "again           Lemmatised word:again           Stemmed word:again          \n",
            "today           Lemmatised word:today           Stemmed word:today          \n",
            ".               Lemmatised word:.               Stemmed word:.              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.It uses the pos_tag function from NLTK to perform POS tagging on the list of words. The Averaged Perceptron tagger model is used to assign a tag to each word. The result is stored in the tagged_words variable."
      ],
      "metadata": {
        "id": "-zXmdwr5J22q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech (POS) tagging is the process of labeling each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc."
      ],
      "metadata": {
        "id": "1uzUuBv9O4qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Kripa Mary Jose is an outstanding student in the field of artificial intelligence and machine learning. With her dedication and passion, she explores the depths of AI and ML, unraveling complex algorithms and techniques. Her innovative mindset and analytical skills set her apart, driving her towards groundbreaking discoveries in the realm of technology. As an AI and ML enthusiast, Kripa Mary Jose continues to inspire her peers with her relentless pursuit of knowledge and her commitment to pushing the boundaries of what is possible in the ever-evolving world of AI and ML.\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "print(tagged_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eDl2fG1JdsQ",
        "outputId": "15f61d21-361d-4e43-dc6e-4586a6d4bb41"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Kripa', 'NNP'), ('Mary', 'NNP'), ('Jose', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('outstanding', 'JJ'), ('student', 'NN'), ('in', 'IN'), ('the', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.'), ('With', 'IN'), ('her', 'PRP$'), ('dedication', 'NN'), ('and', 'CC'), ('passion', 'NN'), (',', ','), ('she', 'PRP'), ('explores', 'VBZ'), ('the', 'DT'), ('depths', 'NNS'), ('of', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('ML', 'NNP'), (',', ','), ('unraveling', 'VBG'), ('complex', 'JJ'), ('algorithms', 'NN'), ('and', 'CC'), ('techniques', 'NNS'), ('.', '.'), ('Her', 'PRP$'), ('innovative', 'JJ'), ('mindset', 'NN'), ('and', 'CC'), ('analytical', 'JJ'), ('skills', 'NNS'), ('set', 'VBD'), ('her', 'PRP'), ('apart', 'NN'), (',', ','), ('driving', 'VBG'), ('her', 'PRP$'), ('towards', 'NNS'), ('groundbreaking', 'VBG'), ('discoveries', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('realm', 'NN'), ('of', 'IN'), ('technology', 'NN'), ('.', '.'), ('As', 'IN'), ('an', 'DT'), ('AI', 'NNP'), ('and', 'CC'), ('ML', 'NNP'), ('enthusiast', 'NN'), (',', ','), ('Kripa', 'NNP'), ('Mary', 'NNP'), ('Jose', 'NNP'), ('continues', 'VBZ'), ('to', 'TO'), ('inspire', 'VB'), ('her', 'PRP$'), ('peers', 'NNS'), ('with', 'IN'), ('her', 'PRP$'), ('relentless', 'NN'), ('pursuit', 'NN'), ('of', 'IN'), ('knowledge', 'NN'), ('and', 'CC'), ('her', 'PRP$'), ('commitment', 'NN'), ('to', 'TO'), ('pushing', 'VBG'), ('the', 'DT'), ('boundaries', 'NNS'), ('of', 'IN'), ('what', 'WP'), ('is', 'VBZ'), ('possible', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('ever-evolving', 'JJ'), ('world', 'NN'), ('of', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('ML', 'NNP'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.The sentence is tokenized and the words are POS tagged and the NER(Named Entity Recognition) is performed using ne_chunk function."
      ],
      "metadata": {
        "id": "LCTRvBqPKpTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER) is a natural language processing task that involves identifying and classifying named entities within text into predefined categories such as names of persons, organizations, locations, dates, and more"
      ],
      "metadata": {
        "id": "0bCnxcN0O-3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "words = word_tokenize(sentence)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "named_entities = ne_chunk(tagged_words)\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zglwKpapKqTT",
        "outputId": "d3559a12-cb96-419b-b8c7-9bea295f617a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Kripa/NNP)\n",
            "  (PERSON Mary/NNP Jose/NNP)\n",
            "  is/VBZ\n",
            "  an/DT\n",
            "  outstanding/JJ\n",
            "  student/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  field/NN\n",
            "  of/IN\n",
            "  artificial/JJ\n",
            "  intelligence/NN\n",
            "  and/CC\n",
            "  machine/NN\n",
            "  learning/NN\n",
            "  ./.\n",
            "  With/IN\n",
            "  her/PRP$\n",
            "  dedication/NN\n",
            "  and/CC\n",
            "  passion/NN\n",
            "  ,/,\n",
            "  she/PRP\n",
            "  explores/VBZ\n",
            "  the/DT\n",
            "  depths/NNS\n",
            "  of/IN\n",
            "  (ORGANIZATION AI/NNP)\n",
            "  and/CC\n",
            "  (ORGANIZATION ML/NNP)\n",
            "  ,/,\n",
            "  unraveling/VBG\n",
            "  complex/JJ\n",
            "  algorithms/NN\n",
            "  and/CC\n",
            "  techniques/NNS\n",
            "  ./.\n",
            "  Her/PRP$\n",
            "  innovative/JJ\n",
            "  mindset/NN\n",
            "  and/CC\n",
            "  analytical/JJ\n",
            "  skills/NNS\n",
            "  set/VBD\n",
            "  her/PRP\n",
            "  apart/NN\n",
            "  ,/,\n",
            "  driving/VBG\n",
            "  her/PRP$\n",
            "  towards/NNS\n",
            "  groundbreaking/VBG\n",
            "  discoveries/NNS\n",
            "  in/IN\n",
            "  the/DT\n",
            "  realm/NN\n",
            "  of/IN\n",
            "  technology/NN\n",
            "  ./.\n",
            "  As/IN\n",
            "  an/DT\n",
            "  AI/NNP\n",
            "  and/CC\n",
            "  (ORGANIZATION ML/NNP)\n",
            "  enthusiast/NN\n",
            "  ,/,\n",
            "  (PERSON Kripa/NNP Mary/NNP Jose/NNP)\n",
            "  continues/VBZ\n",
            "  to/TO\n",
            "  inspire/VB\n",
            "  her/PRP$\n",
            "  peers/NNS\n",
            "  with/IN\n",
            "  her/PRP$\n",
            "  relentless/NN\n",
            "  pursuit/NN\n",
            "  of/IN\n",
            "  knowledge/NN\n",
            "  and/CC\n",
            "  her/PRP$\n",
            "  commitment/NN\n",
            "  to/TO\n",
            "  pushing/VBG\n",
            "  the/DT\n",
            "  boundaries/NNS\n",
            "  of/IN\n",
            "  what/WP\n",
            "  is/VBZ\n",
            "  possible/JJ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  ever-evolving/JJ\n",
            "  world/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION AI/NNP)\n",
            "  and/CC\n",
            "  (ORGANIZATION ML/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parsing involves analyzing the syntactic structure of a sentence by identifying the relationships between words.Constituency parsing involves analyzing the syntactic structure of a sentence by breaking it down into smaller, nested phrases or constituents"
      ],
      "metadata": {
        "id": "iiAfRbtKPNUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "sentences = [\"My name Kripa Mary Jose.I am studying MSAIML\"]\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    doc = nlp(sentence)\n",
        "    print(f\"Depdendency Parse for Sentence {i+1}:\")\n",
        "    displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "WGv0y7eqK0Bb",
        "outputId": "f112e94c-0e5c-4b0b-b76d-23c7cf296b8a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depdendency Parse for Sentence 1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0701fdb6d9dc4095b0908ef8ccf7a2df-0\" class=\"displacy\" width=\"860\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">My</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">name</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">Kripa</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">Mary</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">Jose.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">am</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">studying</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">MSAIML</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-1\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M250,139.0 L242,127.0 258,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-2\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-3\" stroke-width=\"2px\" d=\"M160,137.0 C160,2.0 410.0,2.0 410.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M410.0,139.0 L418.0,127.0 402.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-4\" stroke-width=\"2px\" d=\"M520,137.0 C520,47.0 675.0,47.0 675.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520,139.0 L512,127.0 528,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-5\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M610,139.0 L602,127.0 618,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-6\" stroke-width=\"2px\" d=\"M700,137.0 C700,92.0 760.0,92.0 760.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0701fdb6d9dc4095b0908ef8ccf7a2df-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M760.0,139.0 L768.0,127.0 752.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tree import Tree\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "for sent in doc.sents:\n",
        "    tree = to_nltk_tree(sent.root)\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MCXpitZKnqy",
        "outputId": "74881770-e721-42d6-cbb7-2fc9218b5f0f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    name                \n",
            "  ___|__________         \n",
            " |   |         Jose     \n",
            " |   |      ____|____    \n",
            " My  .   Kripa      Mary\n",
            "\n",
            "    studying       \n",
            "  _____|_______     \n",
            " I     am    MSAIML\n",
            "\n"
          ]
        }
      ]
    }
  ]
}