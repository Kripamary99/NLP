{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION: Tokenization is the process of breaking down a piece of text into smaller units called tokens."
      ],
      "metadata": {
        "id": "adCS-qzwsvid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\" In the vast realm of natural language processing, mastering the art of tokenization is ðŸ”‘. ðŸ“œ Whether we're dissecting complex linguistic structures or decoding casual ðŸ—¨ï¸ conversations, tokenization ðŸ§© lays the groundwork. In this domain, understanding the nuances of various tokenizers is crucial. From the basic word tokenization using Punctuation-based Tokenizer ðŸ› ï¸ to the advanced techniques like spaCy Tokenizer ðŸ¤–, each tool has its place. Don't underestimate the power of Multi-Word Expression Tokenizer ðŸ¤¯, for it navigates through idiomatic phrases with finesse. Even in the concise world of tweets ðŸ¦, the Tweet Tokenizer ðŸ“± reigns supreme, effortlessly segmenting hashtags and mentions. And let's not forget about the Gensim word tokenizer ðŸ“Š, indispensable in the domain of topic modeling. Through tokenization with Keras ðŸ§ , we delve into the neural network's realm, harnessing its capabilities for text analysis. In this intricate dance of words and algorithms, proper tokenization is the first step towards unlocking the secrets of language. ðŸš€\""
      ],
      "metadata": {
        "id": "ukOSk5OGstfH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD TOKENIZATION:\n",
        "\n",
        "\n",
        "Insights: Word tokenization divides a text into individual words or tokens based on space or punctuation boundaries.\n",
        "\n",
        "Applications: Used for counting words, building word clouds, and input preparation for text classification or sentiment analysis."
      ],
      "metadata": {
        "id": "m14Ktu5Rs7py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngdOltlNBP87",
        "outputId": "76573603-d5e8-4121-e578-0c7230b2b131"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordtoken=[]\n",
        "wordtoken =word_tokenize(text)\n",
        "print(\"The word tokenization on text:\" , wordtoken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrp7P8XdJdrW",
        "outputId": "aaf51e0a-f339-4be8-a021-d7cd3f400396"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word tokenization on text: ['In', 'the', 'vast', 'realm', 'of', 'natural', 'language', 'processing', ',', 'mastering', 'the', 'art', 'of', 'tokenization', 'is', 'ðŸ”‘', '.', 'ðŸ“œ', 'Whether', 'we', \"'re\", 'dissecting', 'complex', 'linguistic', 'structures', 'or', 'decoding', 'casual', 'ðŸ—¨ï¸', 'conversations', ',', 'tokenization', 'ðŸ§©', 'lays', 'the', 'groundwork', '.', 'In', 'this', 'domain', ',', 'understanding', 'the', 'nuances', 'of', 'various', 'tokenizers', 'is', 'crucial', '.', 'From', 'the', 'basic', 'word', 'tokenization', 'using', 'Punctuation-based', 'Tokenizer', 'ðŸ› ï¸', 'to', 'the', 'advanced', 'techniques', 'like', 'spaCy', 'Tokenizer', 'ðŸ¤–', ',', 'each', 'tool', 'has', 'its', 'place', '.', 'Do', \"n't\", 'underestimate', 'the', 'power', 'of', 'Multi-Word', 'Expression', 'Tokenizer', 'ðŸ¤¯', ',', 'for', 'it', 'navigates', 'through', 'idiomatic', 'phrases', 'with', 'finesse', '.', 'Even', 'in', 'the', 'concise', 'world', 'of', 'tweets', 'ðŸ¦', ',', 'the', 'Tweet', 'Tokenizer', 'ðŸ“±', 'reigns', 'supreme', ',', 'effortlessly', 'segmenting', 'hashtags', 'and', 'mentions', '.', 'And', 'let', \"'s\", 'not', 'forget', 'about', 'the', 'Gensim', 'word', 'tokenizer', 'ðŸ“Š', ',', 'indispensable', 'in', 'the', 'domain', 'of', 'topic', 'modeling', '.', 'Through', 'tokenization', 'with', 'Keras', 'ðŸ§ ', ',', 'we', 'delve', 'into', 'the', 'neural', 'network', \"'s\", 'realm', ',', 'harnessing', 'its', 'capabilities', 'for', 'text', 'analysis', '.', 'In', 'this', 'intricate', 'dance', 'of', 'words', 'and', 'algorithms', ',', 'proper', 'tokenization', 'is', 'the', 'first', 'step', 'towards', 'unlocking', 'the', 'secrets', 'of', 'language', '.', 'ðŸš€']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTENCE TOKENIZATION:\n",
        "\n",
        "Insights: Sentence tokenization divides a text into individual sentences based on punctuation cues like periods, question marks, and exclamation points.\n",
        "\n",
        " Applications: Used for language modeling, text summarization, and machine translation, where sentences are treated as separate units of analysis."
      ],
      "metadata": {
        "id": "IYtkGfbKtAys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senttoken=[]\n",
        "senttoken=sent_tokenize(text)\n",
        "print(\"The sentence tokenization on text:\" , senttoken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R27sKWPyJuzE",
        "outputId": "eeb09e79-0d0d-48f0-c339-15d5478618d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence tokenization on text: [' In the vast realm of natural language processing, mastering the art of tokenization is ðŸ”‘.', \"ðŸ“œ Whether we're dissecting complex linguistic structures or decoding casual ðŸ—¨ï¸ conversations, tokenization ðŸ§© lays the groundwork.\", 'In this domain, understanding the nuances of various tokenizers is crucial.', 'From the basic word tokenization using Punctuation-based Tokenizer ðŸ› ï¸ to the advanced techniques like spaCy Tokenizer ðŸ¤–, each tool has its place.', \"Don't underestimate the power of Multi-Word Expression Tokenizer ðŸ¤¯, for it navigates through idiomatic phrases with finesse.\", 'Even in the concise world of tweets ðŸ¦, the Tweet Tokenizer ðŸ“± reigns supreme, effortlessly segmenting hashtags and mentions.', \"And let's not forget about the Gensim word tokenizer ðŸ“Š, indispensable in the domain of topic modeling.\", \"Through tokenization with Keras ðŸ§ , we delve into the neural network's realm, harnessing its capabilities for text analysis.\", 'In this intricate dance of words and algorithms, proper tokenization is the first step towards unlocking the secrets of language.', 'ðŸš€']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUNCTUATION- BASED TOKENIZATION:\n",
        "\n",
        "\n",
        "*    Insights: Punctuation-based tokenizers split text based on punctuation marks such as commas, periods, and hyphens.\n",
        "*  Possible Applications: IUsed for extracting lists of items from text, handling punctuation-sensitive tasks like extracting hashtags or email addresses, and preprocessing text for specific NLP tasks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p89lcZqbtCym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "puntoken = WordPunctTokenizer()\n",
        "token=[]\n",
        "token =puntoken.tokenize(text)\n",
        "print(\" The Punctuation based tokenizer on the text:\",token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6iXJwmUJ-N0",
        "outputId": "3beb63ac-68a7-4c21-9319-3d72f8545791"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Punctuation based tokenizer on the text: ['In', 'the', 'vast', 'realm', 'of', 'natural', 'language', 'processing', ',', 'mastering', 'the', 'art', 'of', 'tokenization', 'is', 'ðŸ”‘.', 'ðŸ“œ', 'Whether', 'we', \"'\", 're', 'dissecting', 'complex', 'linguistic', 'structures', 'or', 'decoding', 'casual', 'ðŸ—¨ï¸', 'conversations', ',', 'tokenization', 'ðŸ§©', 'lays', 'the', 'groundwork', '.', 'In', 'this', 'domain', ',', 'understanding', 'the', 'nuances', 'of', 'various', 'tokenizers', 'is', 'crucial', '.', 'From', 'the', 'basic', 'word', 'tokenization', 'using', 'Punctuation', '-', 'based', 'Tokenizer', 'ðŸ› ï¸', 'to', 'the', 'advanced', 'techniques', 'like', 'spaCy', 'Tokenizer', 'ðŸ¤–,', 'each', 'tool', 'has', 'its', 'place', '.', 'Don', \"'\", 't', 'underestimate', 'the', 'power', 'of', 'Multi', '-', 'Word', 'Expression', 'Tokenizer', 'ðŸ¤¯,', 'for', 'it', 'navigates', 'through', 'idiomatic', 'phrases', 'with', 'finesse', '.', 'Even', 'in', 'the', 'concise', 'world', 'of', 'tweets', 'ðŸ¦,', 'the', 'Tweet', 'Tokenizer', 'ðŸ“±', 'reigns', 'supreme', ',', 'effortlessly', 'segmenting', 'hashtags', 'and', 'mentions', '.', 'And', 'let', \"'\", 's', 'not', 'forget', 'about', 'the', 'Gensim', 'word', 'tokenizer', 'ðŸ“Š,', 'indispensable', 'in', 'the', 'domain', 'of', 'topic', 'modeling', '.', 'Through', 'tokenization', 'with', 'Keras', 'ðŸ§ ,', 'we', 'delve', 'into', 'the', 'neural', 'network', \"'\", 's', 'realm', ',', 'harnessing', 'its', 'capabilities', 'for', 'text', 'analysis', '.', 'In', 'this', 'intricate', 'dance', 'of', 'words', 'and', 'algorithms', ',', 'proper', 'tokenization', 'is', 'the', 'first', 'step', 'towards', 'unlocking', 'the', 'secrets', 'of', 'language', '.', 'ðŸš€']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TREEBANKWORD\n",
        "\n",
        "*    Insights: The Treebank tokenizer is a part of NLTK and tokenizes text according to the conventions of the Penn Treebank corpus.\n",
        "* Possible Applications: used for parsing and tagging text in various NLP tasks such as part-of-speech tagging, named entity recognition, and syntactic analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RJFVEycStIjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "treebank = TreebankWordTokenizer()\n",
        "token=[]\n",
        "token = treebank.tokenize(text)\n",
        "print(\" The Tree word  tokenizer on the text:\",token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrQmRDtiO19a",
        "outputId": "ed836df2-2b0b-4c02-ea77-046ce6218575"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Tree word  tokenizer on the text: ['In', 'the', 'vast', 'realm', 'of', 'natural', 'language', 'processing', ',', 'mastering', 'the', 'art', 'of', 'tokenization', 'is', 'ðŸ”‘.', 'ðŸ“œ', 'Whether', 'we', \"'re\", 'dissecting', 'complex', 'linguistic', 'structures', 'or', 'decoding', 'casual', 'ðŸ—¨ï¸', 'conversations', ',', 'tokenization', 'ðŸ§©', 'lays', 'the', 'groundwork.', 'In', 'this', 'domain', ',', 'understanding', 'the', 'nuances', 'of', 'various', 'tokenizers', 'is', 'crucial.', 'From', 'the', 'basic', 'word', 'tokenization', 'using', 'Punctuation-based', 'Tokenizer', 'ðŸ› ï¸', 'to', 'the', 'advanced', 'techniques', 'like', 'spaCy', 'Tokenizer', 'ðŸ¤–', ',', 'each', 'tool', 'has', 'its', 'place.', 'Do', \"n't\", 'underestimate', 'the', 'power', 'of', 'Multi-Word', 'Expression', 'Tokenizer', 'ðŸ¤¯', ',', 'for', 'it', 'navigates', 'through', 'idiomatic', 'phrases', 'with', 'finesse.', 'Even', 'in', 'the', 'concise', 'world', 'of', 'tweets', 'ðŸ¦', ',', 'the', 'Tweet', 'Tokenizer', 'ðŸ“±', 'reigns', 'supreme', ',', 'effortlessly', 'segmenting', 'hashtags', 'and', 'mentions.', 'And', 'let', \"'s\", 'not', 'forget', 'about', 'the', 'Gensim', 'word', 'tokenizer', 'ðŸ“Š', ',', 'indispensable', 'in', 'the', 'domain', 'of', 'topic', 'modeling.', 'Through', 'tokenization', 'with', 'Keras', 'ðŸ§ ', ',', 'we', 'delve', 'into', 'the', 'neural', 'network', \"'s\", 'realm', ',', 'harnessing', 'its', 'capabilities', 'for', 'text', 'analysis.', 'In', 'this', 'intricate', 'dance', 'of', 'words', 'and', 'algorithms', ',', 'proper', 'tokenization', 'is', 'the', 'first', 'step', 'towards', 'unlocking', 'the', 'secrets', 'of', 'language.', 'ðŸš€']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TWEET TOKENIZATION:\n",
        "\n",
        "\n",
        "*  Insights: Tweet tokenization is specifically designed to handle text from social media platforms like Twitter.\n",
        "*   Possible Applications: used for sentiment analysis of tweets, extracting user mentions and hashtags, and analyzing social media trends.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bMMLYc8KtM7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample =\"ðŸŽ‰ Just booked my flight âœˆï¸ to #Paris! Can't wait to explore the city of love â¤ï¸ #travel #excited\""
      ],
      "metadata": {
        "id": "SiPeJmuBSu9Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  nltk.tokenize import TweetTokenizer\n",
        "tweettoken = TweetTokenizer()\n",
        "token = tweettoken.tokenize(sample)\n",
        "print(\" The Tweet tokenizer on the text:\",token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_x7mzpVTMMA",
        "outputId": "b54d4d1b-4000-4fba-dbfc-e340dea84e77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Tweet tokenizer on the text: ['ðŸŽ‰', 'Just', 'booked', 'my', 'flight', 'âœˆ', 'ï¸', 'to', '#Paris', '!', \"Can't\", 'wait', 'to', 'explore', 'the', 'city', 'of', 'love', 'â¤', 'ï¸', '#travel', '#excited']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI WORD TOKENIZER:\n",
        "\n",
        "\n",
        "\n",
        "*   Insights: This tokenizer identifies and treats multi-word expressions as single tokens during tokenization.\n",
        "*   Possible Applications: Usedd to preserve the meaning of idiomatic phrases.\n",
        "\n"
      ],
      "metadata": {
        "id": "_1XRUF0xtWVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "text =\" N E W BE GINNING OF LIFE\"\n",
        "mwe = MWETokenizer([('N','E','W'),('BE','GINNING')])\n",
        "token = mwe.tokenize(text.split())\n",
        "print(\" The Multi-Word Expression Tokenizer on the text:\",token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRvq_xhZOtuQ",
        "outputId": "096dbe49-57a3-4ab4-de6e-60772610c5c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Multi-Word Expression Tokenizer on the text: ['N_E_W', 'BE_GINNING', 'OF', 'LIFE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXTBLOB:\n",
        "\n",
        " Insights: TextBlob's word tokenizer\n",
        "tokenizes text into words using a simple space-based approach.\n",
        "\n",
        "  Possible Applications: It is suitable for basic text processing tasks such as word counting, word frequency analysis, and building simple language models."
      ],
      "metadata": {
        "id": "jTwejILNtZqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U textblob\n",
        "! python -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "KRo8akdb-aXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bea3ee2-6282-4e01-c252-24118f8f0630"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"TextBlob is a simple and powerful Python library for natural language processing (NLP). It provides an easy-to-use interface for tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. With TextBlob, you can quickly analyze and process text data in Python, making it a valuable tool for NLP researchers, developers, and data scientists. Whether you're working on sentiment analysis of customer reviews, categorizing news articles, or building a chatbot, TextBlob can help you efficiently handle various NLP tasks.\"\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(text)\n",
        "print(\"Word tokenization using TextBlob:\",blob.words)\n",
        "print(\"Sentence tokenization using TextBlob:\",blob.sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-s7eWFrjiYc",
        "outputId": "8ad6a55e-7e90-4ba6-a394-46607ba5c859"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenization using TextBlob: ['TextBlob', 'is', 'a', 'simple', 'and', 'powerful', 'Python', 'library', 'for', 'natural', 'language', 'processing', 'NLP', 'It', 'provides', 'an', 'easy-to-use', 'interface', 'for', 'tasks', 'such', 'as', 'part-of-speech', 'tagging', 'noun', 'phrase', 'extraction', 'sentiment', 'analysis', 'classification', 'translation', 'and', 'more', 'With', 'TextBlob', 'you', 'can', 'quickly', 'analyze', 'and', 'process', 'text', 'data', 'in', 'Python', 'making', 'it', 'a', 'valuable', 'tool', 'for', 'NLP', 'researchers', 'developers', 'and', 'data', 'scientists', 'Whether', 'you', \"'re\", 'working', 'on', 'sentiment', 'analysis', 'of', 'customer', 'reviews', 'categorizing', 'news', 'articles', 'or', 'building', 'a', 'chatbot', 'TextBlob', 'can', 'help', 'you', 'efficiently', 'handle', 'various', 'NLP', 'tasks']\n",
            "Sentence tokenization using TextBlob: [Sentence(\"TextBlob is a simple and powerful Python library for natural language processing (NLP).\"), Sentence(\"It provides an easy-to-use interface for tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\"), Sentence(\"With TextBlob, you can quickly analyze and process text data in Python, making it a valuable tool for NLP researchers, developers, and data scientists.\"), Sentence(\"Whether you're working on sentiment analysis of customer reviews, categorizing news articles, or building a chatbot, TextBlob can help you efficiently handle various NLP tasks.\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPACY :\n",
        "\n",
        " Insights: spaCy's tokenizer is highly customizable and efficient, capable of tokenizing complex text in multiple languages.\n",
        "\n",
        "  Possible Applications: It is used for a wide range of NLP tasks including named entity recognition, dependency parsing, and information extraction from unstructured text data.\n"
      ],
      "metadata": {
        "id": "-wLus37atbyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spac = spacy.load(\"en_core_web_sm\")\n",
        "doc = spac(text)\n",
        "for token in doc:\n",
        " print(token,\"|\",spacy.explain(token.pos_),\"|\",token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwOaDn6lkRxX",
        "outputId": "fba9f7f3-7c76-40f0-ebf3-e7caa5e41765"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob | proper noun | TextBlob\n",
            "is | auxiliary | be\n",
            "a | determiner | a\n",
            "simple | adjective | simple\n",
            "and | coordinating conjunction | and\n",
            "powerful | adjective | powerful\n",
            "Python | proper noun | Python\n",
            "library | noun | library\n",
            "for | adposition | for\n",
            "natural | adjective | natural\n",
            "language | noun | language\n",
            "processing | noun | processing\n",
            "( | punctuation | (\n",
            "NLP | proper noun | NLP\n",
            ") | punctuation | )\n",
            ". | punctuation | .\n",
            "It | pronoun | it\n",
            "provides | verb | provide\n",
            "an | determiner | an\n",
            "easy | adjective | easy\n",
            "- | punctuation | -\n",
            "to | adposition | to\n",
            "- | punctuation | -\n",
            "use | noun | use\n",
            "interface | noun | interface\n",
            "for | adposition | for\n",
            "tasks | noun | task\n",
            "such | adjective | such\n",
            "as | adposition | as\n",
            "part | noun | part\n",
            "- | punctuation | -\n",
            "of | adposition | of\n",
            "- | punctuation | -\n",
            "speech | noun | speech\n",
            "tagging | noun | tagging\n",
            ", | punctuation | ,\n",
            "noun | proper noun | noun\n",
            "phrase | noun | phrase\n",
            "extraction | noun | extraction\n",
            ", | punctuation | ,\n",
            "sentiment | noun | sentiment\n",
            "analysis | noun | analysis\n",
            ", | punctuation | ,\n",
            "classification | noun | classification\n",
            ", | punctuation | ,\n",
            "translation | noun | translation\n",
            ", | punctuation | ,\n",
            "and | coordinating conjunction | and\n",
            "more | adjective | more\n",
            ". | punctuation | .\n",
            "With | adposition | with\n",
            "TextBlob | proper noun | TextBlob\n",
            ", | punctuation | ,\n",
            "you | pronoun | you\n",
            "can | auxiliary | can\n",
            "quickly | adverb | quickly\n",
            "analyze | verb | analyze\n",
            "and | coordinating conjunction | and\n",
            "process | verb | process\n",
            "text | noun | text\n",
            "data | noun | datum\n",
            "in | adposition | in\n",
            "Python | proper noun | Python\n",
            ", | punctuation | ,\n",
            "making | verb | make\n",
            "it | pronoun | it\n",
            "a | determiner | a\n",
            "valuable | adjective | valuable\n",
            "tool | noun | tool\n",
            "for | adposition | for\n",
            "NLP | proper noun | NLP\n",
            "researchers | noun | researcher\n",
            ", | punctuation | ,\n",
            "developers | noun | developer\n",
            ", | punctuation | ,\n",
            "and | coordinating conjunction | and\n",
            "data | noun | datum\n",
            "scientists | noun | scientist\n",
            ". | punctuation | .\n",
            "Whether | subordinating conjunction | whether\n",
            "you | pronoun | you\n",
            "'re | auxiliary | be\n",
            "working | verb | work\n",
            "on | adposition | on\n",
            "sentiment | noun | sentiment\n",
            "analysis | noun | analysis\n",
            "of | adposition | of\n",
            "customer | noun | customer\n",
            "reviews | noun | review\n",
            ", | punctuation | ,\n",
            "categorizing | verb | categorize\n",
            "news | noun | news\n",
            "articles | noun | article\n",
            ", | punctuation | ,\n",
            "or | coordinating conjunction | or\n",
            "building | verb | build\n",
            "a | determiner | a\n",
            "chatbot | noun | chatbot\n",
            ", | punctuation | ,\n",
            "TextBlob | proper noun | TextBlob\n",
            "can | auxiliary | can\n",
            "help | verb | help\n",
            "you | pronoun | you\n",
            "efficiently | adverb | efficiently\n",
            "handle | verb | handle\n",
            "various | adjective | various\n",
            "NLP | proper noun | NLP\n",
            "tasks | noun | task\n",
            ". | punctuation | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENSIM\n",
        "\n",
        " Insights: Gensim is an open-source library for unsupervised topic modeling, document indexing, retrieval by similarity, and other natural language processing functionalities, using modern statistical machine learning\n",
        "\n",
        "  Possible Applications: It is suitable for training word embedding models"
      ],
      "metadata": {
        "id": "zesYosphtfJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "tokens = list(tokenize(text))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zcZmZo-nDAc",
        "outputId": "ff61e354-03f9-43e5-900b-41669d24e085"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TextBlob', 'is', 'a', 'simple', 'and', 'powerful', 'Python', 'library', 'for', 'natural', 'language', 'processing', 'NLP', 'It', 'provides', 'an', 'easy', 'to', 'use', 'interface', 'for', 'tasks', 'such', 'as', 'part', 'of', 'speech', 'tagging', 'noun', 'phrase', 'extraction', 'sentiment', 'analysis', 'classification', 'translation', 'and', 'more', 'With', 'TextBlob', 'you', 'can', 'quickly', 'analyze', 'and', 'process', 'text', 'data', 'in', 'Python', 'making', 'it', 'a', 'valuable', 'tool', 'for', 'NLP', 'researchers', 'developers', 'and', 'data', 'scientists', 'Whether', 'you', 're', 'working', 'on', 'sentiment', 'analysis', 'of', 'customer', 'reviews', 'categorizing', 'news', 'articles', 'or', 'building', 'a', 'chatbot', 'TextBlob', 'can', 'help', 'you', 'efficiently', 'handle', 'various', 'NLP', 'tasks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KERAS:\n",
        "\n",
        "Insights: Keras provides a simple interface for tokenizing text data and converting it into sequences of integers.\n",
        "\n",
        "  Possible Applications: It is commonly used for preparing text data for input to neural network models.\n"
      ],
      "metadata": {
        "id": "qh_-s9KvtiA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "token = Tokenizer(num_words=30)\n",
        "token.fit_on_texts(text)\n",
        "token.word_index"
      ],
      "metadata": {
        "id": "CvQ25W52ofOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d23cf5-ecf0-4918-ca5f-507b08bd30d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1,\n",
              " 'e': 2,\n",
              " 't': 3,\n",
              " 'n': 4,\n",
              " 's': 5,\n",
              " 'i': 6,\n",
              " 'o': 7,\n",
              " 'r': 8,\n",
              " 'l': 9,\n",
              " 'c': 10,\n",
              " 'p': 11,\n",
              " 'u': 12,\n",
              " 'y': 13,\n",
              " 'h': 14,\n",
              " 'g': 15,\n",
              " 'b': 16,\n",
              " 'd': 17,\n",
              " 'f': 18,\n",
              " 'm': 19,\n",
              " 'w': 20,\n",
              " 'x': 21,\n",
              " 'v': 22,\n",
              " 'k': 23,\n",
              " 'z': 24,\n",
              " 'q': 25,\n",
              " \"'\": 26}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jzjzvB2bso9N"
      }
    }
  ]
}